#=
To Do:
    1: Make this square root form work for linear systems too, now that we split up predict and update
    2: Add the "observation reduction" function to modify the observer to allign with finite values of y 
       This model reduction should be automatic. 
       Since it operates on the same states, updating the modified problem should update the original
=#

using LinearAlgebra
import Statistics.mean
import Statistics.cov

"""
Sigma point parameters for the unscented kalman filter (Î±~0 => EKF Î±~1=> Nonlinear Gaussian, Îº=0, Î²=2 for Gaussian)
"""
Base.@kwdef struct SigmaParams
    Î± :: Float64 = 0.001
    Îº :: Float64 = 0.0
    Î² :: Float64 = 2.0
end

LinearPredictor = Tuple{<:AbstractArray,<:AbstractArray}
StatePredictor  = Union{Function, LinearPredictor}

"""
State-Space model
Fields:   
    fxu: state transition function f of inputs x and u (or a tuple of two matrices for the linear version)
    hxu: state observation function h of inputs x and u (or a tuple of two matrices for the linear version)
    x: "a priori" state estimate
    PU: "a priori" estimated state covariance (Upper-Triangular form)
    QU: process noise covariance (Upper-Triangular form)
    RU: measurement noise covariance (Upper-Triangular form)
"""
Base.@kwdef struct StateSpaceModel{T<:Real, F1<:StatePredictor, F2<:StatePredictor}
    fxu ::   F1
    hxu ::   F2
    x   ::   Vector{T}
    QU  ::   UpperTriangular{T, Matrix{T}}
    RU  ::   UpperTriangular{T, Matrix{T}}
    PU  ::   UpperTriangular{T, Matrix{T}}
    Î¸   ::   SigmaParams = SigmaParams()
end

function StateSpaceModel(fx::F1, hx::F2, x, QU, RU, PU, Î¸) where {F1,F2}
    T = promote_type(eltype(x), eltype(QU), eltype(RU), eltype(PU))
    return StateSpaceModel{T, F1, F2}(fx, hx, x, QU, RU, PU, Î¸)
end

"""
GaussianState: contains the canonical representation of a model's state (x = state, P = state covariance)
"""
@kwdef struct GaussianState
    x :: Vector{Float64}
    P :: Matrix{Float64}
end

function GaussianState(model::StateSpaceModel)
    return GaussianState(
        x = deepcopy(model.x),
        P = model.PU'model.PU
    )
end

"""
kalman_filter!(SS::StateSpaceModel{T}, y::AbstractVector, u; multithreaded_predict=false, multithreaded_observe=false, outlier=3.0) where T

Applies state prediction and update functionality in-place to the state space model (does not update if there are NaN values)
Output: Primary output mutates state-space model fields (x, PU),
Additionaly, a NamedTuple is provided with the following fields:
    xh: State estimate before the update (for troubleshooting)
    Ph: State covariance before the update (for troubleshooting)
    yh: Predicted observation (for model validation)
    K:  Kalman gain (for troubleshooting)
"""
function kalman_filter!(SS::StateSpaceModel{T}, y::AbstractVector, u; multithreaded_predict=false, multithreaded_observe=false, outlier=3.0) where T
    #Propagate sigma points through transition
    TR = promote_type(T, Float64)
    (xh, Ph) = predict_state!(SS, u, multithreaded=multithreaded_predict)
    (yh, K)  = update_state!(SS, y, u,  multithreaded=multithreaded_observe, outlier=outlier)

    return (xh=xh, Ph=Ph, yh=yh, K=K)
end



"""
Weights for sigma points, calculated from SigmaParams and the state dimension L
"""
Base.@kwdef struct SigmaWeights
    c :: Float64
    Î¼ :: Tuple{Float64, Float64}
    Î£ :: Tuple{Float64, Float64}
end

function SigmaWeights(L::Int64, Î¸::SigmaParams=SigmaParams())
    Î± = Î¸.Î±
    Îº = Î¸.Îº
    Î² = Î¸.Î²

    Î»  = Î±^2*(L+Îº)-L                          #scaling factor
    c  = L + Î»                                #scaling factor
    Wn = 0.5/c
    WÎ¼ = ((Î»/c), Wn)                         #weights for means
    WÎ£ = (WÎ¼[1] + (1-Î±^2+Î²), Wn)             #weights for covariance
    return SigmaWeights(c=c, Î¼=WÎ¼, Î£=WÎ£)
end

function SigmaWeights(SS::StateSpaceModel)
    return SigmaWeights(length(SS.x), SS.Î¸)
end


"""
Sigma point generation (including weights)
"""
Base.@kwdef struct SigmaPoints{T}
    points   :: Matrix{T}
    weights  :: SigmaWeights
end

function SigmaPoints(x::AbstractVector{T}, L::LowerTriangular, w::SigmaWeights) where T<:Real
    A = sqrt(w.c)*L
    points = [(x) (x .+ A) (x .- A)]
    return SigmaPoints(points=points, weights=w)
end
SigmaPoints(M::AbstractMatrix{T}, w::SigmaWeights) where T = SigmaPoints{T}(M, w)
SigmaPoints(x::AbstractVector{T}, R::UpperTriangular, w::SigmaWeights) where T<:Real = SigmaPoints(x, R', w)
SigmaPoints(x::AbstractVector{T}, C::Cholesky, w::SigmaWeights) where T<:Real = SigmaPoints(x, C.L, w)



"""
In-place state prediction with result-checking; returns intermediate results for troubleshooting
"""
function predict_state!(SS::StateSpaceModel{<:Real, <:Any, <:Any}, u; multithreaded=false)
    #Propagate sigma points through transition
    (xh, Ph) = predict_state(SS, u, multithreaded=multithreaded)

    #Overwrite the current state space model
    if any(isnan, xh) | any(isnan, Ph.U)
        @warn "predict_state! Warning: NaN detected in state, skipping"
    else
        SS.x  .= xh 
        SS.PU .= Ph.U
    end
    return (xh=xh, Ph=Ph)
end

"""
Predict the observation given a state
"""
predict_observation(SS::StateSpaceModel{<:Real, <:Any, <:LinearPredictor}, u) = SS.hxu[1]*SS.x + SS.hxu[2]*u
predict_observation(SS::StateSpaceModel{<:Real, <:Any, <:Function}, u) = SS.hxu(SS.x, u)

"""
In-place state update with automatic handling of missing observations; returns intermediate results for troubleshooting
"""
function update_state!(SS::StateSpaceModel{T, <:Any, <:Any}, y, u; multithreaded=false, outlier=3.0) where T <: Real
    
    if !all(isfinite, y) #Remove NaNs/Infs from the observations and model and call again
        ind = isfinite.(y)
        SR  = reduce_observer(SS, ind)
        return update_state!(SR, y[ind], u, multithreaded=multithreaded, outlier=outlier)
   
    elseif isempty(y) #No valid observations
        @warn "update_state! Warning: no observations, skipping update"
        TR = promote_type(T,Float64)
        return (yh=TR[], K=zeros(TR, length(SS.x), 0))
    end

    #Update state space model objects
    OBS = update_state(SS, y, u, multithreaded=multithreaded, outlier=outlier)
    if any(isnan, OBS.xh) | any(isnan, OBS.Ph.U)
        @warn "update_state! Warning: NaN detected in state, skipping"
    else
        SS.x  .= OBS.xh
        SS.PU .= OBS.Ph.U
    end
    return (yh=OBS.yh, K=OBS.K)
end

"""
Nonlinear state prediction
"""
function predict_state(SS::StateSpaceModel{<:Real, <:Function, <:Any}, u; multithreaded=false)
    #Propagate sigma points through transition
    w = SigmaWeights(SS)
    ð’³t = SigmaPoints(SS.x, SS.PU, w)
    ð’³t = predict!(SS.fxu, ð’³t, u, multithreaded=multithreaded)
    
    #Obtain prediction covariance
    xh = mean(ð’³t)
    Ph = chol_update(SS.QU, subtract(ð’³t, xh))
    return (xh=xh, Ph=Ph)
end

"""
Linear state prediction
"""
function predict_state(SS::StateSpaceModel{<:Real, <:LinearPredictor, <:Any}, u; multithreaded=false)
    (A, B) = (SS.fxu[1], SS.fxu[2])
    xh = A*SS.x + B*u
    Ph = Cholesky(root_sum_squared(SS.PU*A', SS.QU), :U, 0)
    return (xh=xh, Ph=Ph)
end


"""
Nonlinear state updating
"""
function update_state(SS::StateSpaceModel{<:Real, <:Any, <:Function}, y, u; multithreaded=false, outlier=3.0)
    w = SigmaWeights(SS)

    #Propagate new predicted sigma points though observation
    ð’³ = SigmaPoints(SS.x, SS.PU, w)
    ð’´ = predict(SS.hxu, ð’³, u, multithreaded=multithreaded)
    yh = mean(ð’´)
    
    S = chol_update(SS.RU, subtract(ð’´, yh)) #Obtain cholesky of the innovation covariance
    Pxy = cov(ð’³, ð’´) #Obtain cross-covariance of state and measurement innovations
    K = (Pxy/(S.U))/S.L #Kalman gain

    Ïƒz = chol_std(S)
    xh = SS.x .+ K*scale_innovation.(y.-yh, Ïƒz, outlier=outlier)
    Ph = chol_update!(Cholesky(copy(SS.PU),:U,0), K*S.L, -1)

    return (xh=xh, Ph=Ph, yh=yh, K=K)
end

"""
Linear state updating
"""
function update_state(SS::StateSpaceModel{<:Real, <:Any, <:LinearPredictor}, y, u; multithreaded=false, outlier=3.0)
    (C, D) = (SS.hxu[1], SS.hxu[2])
    yh = C*SS.x .+ D*u

    S = Cholesky(root_sum_squared(SS.PU*C', SS.RU)) #Obtain cholesky of the innovation covariance
    Pxy = (SS.PU'*SS.PU)*C' #Obtain cross-covariance of state and measurement innovations
    K = (Pxy/(S.U))/S.L #Kalman gain

    Ïƒz = chol_std(S)
    xh = SS.x .+ K*scale_innovation.(y.-yh, Ïƒz, outlier=outlier)
    Ph = root_sum_squared(SS.PU*(I-K*C)', SS.RU*K')

    return (xh=xh, Ph=Cholesky(Ph,:U,0), yh=yh, K=K)
end


"""
Retruns a state space model with a reduced observer space determined by the second argument "not_missing", 
This is useful for handling missing data (where the non-missing elements are defined by "not_missing" )
"""
function reduce_observer(SS::StateSpaceModel{T, <:Any, <:Function}, not_missing) where T
    reduced_obsfunc(x,u) = SS.hxu(x,u)[not_missing]
    return StateSpaceModel{T, typeof(SS.fxu), typeof(reduced_obsfunc)}(
        fxu = SS.fxu,
        hxu = reduced_obsfunc,
        x  = SS.x,
        QU = SS.QU,
        RU = UpperTriangular(SS.RU[not_missing, not_missing]),
        PU = SS.PU,
        Î¸  = SS.Î¸
    )
end

function reduce_observer(SS::StateSpaceModel{T, <:Any, <:LinearPredictor}, not_missing) where T
    return StateSpaceModel{T, typeof(SS.fxu), typeof(SS.hxu)}(
        fxu = SS.fxu,
        hxu = (SS.hxu[1][not_missing,:], SS.hxu[2][not_missing,:]),
        x  = SS.x,
        QU = SS.QU,
        RU = UpperTriangular(SS.RU[not_missing, not_missing]),
        PU = SS.PU,
        Î¸  = SS.Î¸
    )
end



"""
In-place modification of sigma points ð’³ by applying f to each element. Can be multithreaded if f is computationally intense
"""
function predict!(f::Function, ð’³::SigmaPoints{T}, u; multithreaded=false) where T
    X = ð’³.points
    if multithreaded
        Threads.@threads for ii in axes(X,2)
            X[:,ii] = f(view(X,:,ii), u)
        end
    else
        for ii in axes(X,2)
            X[:,ii] = f(view(X,:,ii), u)
        end
    end
    return ð’³
end

"""
Produces a new set of sigma points by applying f to all sigma points in ð’³ Can be multithreaded if f is computationally intense
"""
function predict(f::Function, ð’³::SigmaPoints{T}, u; multithreaded=false) where T
    if multithreaded
        predictions = [Threads.@spawn f(x, u) for x in eachcol(ð’³.points)]
        return SigmaPoints(points = mapreduce(fetch, hcat, predictions), weights = ð’³.weights)
    else
        predictions = (f(x, u) for x in eachcol(ð’³.points))
        return SigmaPoints(points = reduce(hcat, predictions), weights = ð’³.weights)
    end
end

"""
Returns âˆš(AÂ² + BÂ²) for matrices A and B where âˆšM is the upper triangular square-root of matrix M
"""
function root_sum_squared(A::AbstractMatrix, B::AbstractMatrix)
    R = UpperTriangular(qr!([A;B]).R)
    if any(c->R[c]<0, diagind(R))
        R .= .-R
    end
    return R
end


"""
Produces a new set of sigma points by subtracting x from each point
"""
function subtract(ð’³::SigmaPoints, x::AbstractVector)
    return SigmaPoints(
        points  = ð’³.points .- x,
        weights = ð’³.weights
    )
end


"""
Returns a weighted mean vector of a set of sigma points
"""
function mean(ð’³::SigmaPoints{T}) where T
    weight(ii::Integer) = ifelse(ii==1, ð’³.weights.Î¼[1], ð’³.weights.Î¼[2])

    Î¼  = zeros(promote_type(T, Float64), size(ð’³.points,1))
    ii = 0
    for x in eachcol(ð’³.points)
        ii += 1
        Î¼ .+= weight(ii) .* x
    end
    return Î¼
end


"""
Returns a weighted covariance matrix of two sets of sigma points, based on weights from the first set
"""
function cov(ð’³::SigmaPoints{T1}, ð’´::SigmaPoints{T2}; centered=false) where {T1, T2}
    weight(ii::Integer) = ifelse(ii==1, ð’³.weights.Î£[1], ð’³.weights.Î£[2])

    if size(ð’³.points, 2) != size(ð’´.points, 2)
        error("Two sets of sigma points must have the same number of points")
    end

    #Caclulate the mean
    (Î¼x, Î¼y) = centered ? (0.0, 0.0) : (mean(ð’³), mean(ð’´))
    
    #Fill out the covariance matrix
    T = promote_type(Float64, T1, T2)
    S = zeros(T, size(ð’³.points,1), size(ð’´.points,1))
    ii = 0
    for (x, y) in zip(eachcol(ð’³.points), eachcol(ð’´.points))
        ii += 1
        S .+= weight(ii) .* (x.-Î¼x) .* (y.-Î¼y)'
    end
    return S
end

cov(ð’³::SigmaPoints) = cov(ð’³,ð’³)


#Updating of cholesky objects
"""
Returns the cholesky decopmposition of the sum of two covariance matrices S1, S2 where
S1 is given as a Cholesky Decomposition
S2 is given as a set of sigma points
"""
function chol_update(S1::Cholesky, ð’®2::SigmaPoints)
    return chol_update(S1.L, ð’®2)
end

#The QR decomposition method for updating cholesky decompositions with sigma points stems from some very clever Linear Algebra
#Our target is to calculate the cholesky decomposition of S3 = S1 + S2 where S2 is given as sigma points (ð’®2)
#S2 = (sqrt(w2) ð’®2)*(sqrt(w2) ð’®2)' where ð’®2 are the sigma points of S2 and w2 are the weights
#S2 = A*A' = [(sqrt(w2)*ð’®2)  âˆšS1]*[(sqrt(w2)*ð’®2)  âˆšS1]' where âˆšS1 is the left-square root (lower triangular of the cholesky)
#By doing QR decomposition on A' so that A' = Qâ‚Râ‚, (where Qâ‚ is an orthogonal matrix, Qâ‚*Qâ‚'=I) we get
#S2 = (Qâ‚Râ‚)'(Qâ‚Râ‚) = (Râ‚'Qâ‚')(Qâ‚Râ‚) = Râ‚'Râ‚
#Thus, Râ‚ is the upper-right cholesky decomposition of S3
#The reason why the central sigma point left out of the QR decompodition is that w0 can be negative, which throws everything off 
#The first sigma point is therefore accounted for though the chol_update! (low-rank downdate) which can handle negative w0

function chol_update(SL1::LowerTriangular, ð’®2::SigmaPoints)
    (w0, w1) = (ð’®2.weights.Î£[1], ð’®2.weights.Î£[2])
    A   = @views [(sqrt(w1).*ð’®2.points[:, 2:end]) SL1]
    SCâº = Cholesky(qr(A').R, :U, 0)
    return chol_update!(SCâº, view(ð’®2.points,:,1), w0)
end

chol_update(SR1::UpperTriangular, ð’®2::SigmaPoints) = chol_update(SR1', ð’®2)


function chol_update!(ch::Cholesky, x::AbstractVector, w::Real)
    if w >= 0
        return lowrankupdate!(ch, sqrt(w)*x)
    else
        return lowrankdowndate!(ch, sqrt(abs(w))*x)
    end
end

function chol_update!(ch::Cholesky, X::AbstractArray{<:Real,2}, w::Real)
    for x in eachcol(X)
        chol_update!(ch, x, w)
    end
    return ch
end

function chol_std(ch::Cholesky)
    selfdot(x) = dot(x,x)
    return [sqrt(selfdot(col)) for col in eachcol(ch.U)]
end

#Option to limit transition differences on the state (to avoid chasing outlisers)
function limit_diff!(SS::StateSpaceModel, xh; Sigmas=10.0)
    Î”x = Sigmas.*diag(SS.sQ) 
    SS.x .= clamp.(SS.x, xh.-Î”x, xh.+Î”x)
    return SS
end

#Scale the innoviation to avoid chasing outliers
function scale_innovation(Î”y::Real, Ïƒy::Real; outlier)
    ÏƒÎµ = (outlier/3)*Ïƒy
    return asinh(Î”y/ÏƒÎµ)*ÏƒÎµ
end





# =========================================================================================================
# Legacy code
# =========================================================================================================
#=
#Sigma points around reference point
function sigmas(x0::AbstractVector, L::LowerTriangular, c)
    x = copy(x0)
    N = length(x0)
    A = sqrt(c)*L

    x_plus_A  = [x + A[:,k] for k in 1:N]
    x_minus_A = [x - A[:,k] for k in 1:N]
    return [[x]; x_plus_A; x_minus_A]
end

#Weights of sigma points
function sigma_weights(SS::StateSpaceModel)
    Î± = SS.Î±
    Îº = SS.Îº
    Î² = SS.Î²
    L = length(SS.x)

    Î» = Î±^2*(L+Îº)-L                           #scaling factor
    c = L + Î»                                 #scaling factor
    W  = fill( 0.5/c , 2*L )
    Wm = [(Î»/c) ; W]                          #weights for means
    Wc = [Wm[1] + (1-Î±^2+Î²) ; W]              #weights for covariance

    return c, Wm, Wc
end




#Multithreaded function sampling
function sample_func(f::Function, X::Vector)
    FuncTasks = [Threads.@spawn f(x) for x in X]
    return fetch.( FuncTasks)
end

#Weighted statistics
function sigma_mean(X::Vector, Wm::Vector{<:Real})
    return sum(Wm .* X)
end

#Uses unscented method transform to produce a covariance
function sigma_cov(Î”X::Vector, Î”Y::Vector, Wc::Vector{<:Real})
    CovTasks = [Threads.@spawn Wc[ii]*Î”X[ii]*Î”Y[ii]' for ii in 1:length(Wc) ]
    return sum( fetch.(CovTasks) )
end

#Uses outputs a cholseky decomposition of sigma points plus a diagonal covariance square root
function chol_sigmas_plus_cov(Î”X::Vector, Wc::Vector{<:Real}, sQ::LowerTriangular)
    N = length(Î”X[1])

    #Return first square of the QR factorization of this wide matrix
    sigmaTall = Array( [hcat( sqrt(Wc[2]).*Î”X[2:end]... ) sQ]' )
    ch = Cholesky(qr( sigmaTall ).R, :U, 0)
    return chol_update!(ch, Î”X[1], Wc[1])
end
=#